Deploy:

Notes for DL:
1.  layers.SeparableConv2D() is much leaner than layers.Conv2D() and it does not necessarily 
    perform worse. In fact, it might perform better.

2.  We don't need the bias vector if BatchNormalization is used because it normalizes the output
    of the previous layer, essentially rendering the bias useless. it's good practice to put the
    Activation layer after the BatchNormalization layer. E.g.:
    x = layers.SeparableConv2D(size, 3, padding="same", use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)

3.  We standardize/normalize so that each feature contribute the same amount to gradient descent.
    We use standardization if the assumed underlying distribution is Gaussian.

4.  K-fold validation and iterative k-fold is useful when we have very little data.

5.  Binary classification - binary cross entropy;
    Multi-class classification - categorical cross entropy;
    Multi-label classification - binary cross entropy.

6.  [1,0,0], [0,1,0], [0,0,1] - categorical_crossentropy;
    [1], [2], [3] - sparse_categorical_crossentropy.

7.  Word embeddings are much more efficient that simply one-hot encoded vectors for text.

8.  Factoring outputs into multiple independent spaces, adding residual connections, adding normalization layers
    should be leveraged in any complex model.

9.  Use LayerNormalization instead of BatchNormalization for sequence data.

10. if numSample/meanSampleLength > 1500: Use sequence models. Else: Use bag-of-bigrams models.